dGP is defined to be the amount of games played in the target year above the average number of games played in the previous four seasons. Null values are omitted and not treated as 0. Making dGP the target variable instead of GP improved model accuracy.

Be careful when considering features other than age, height, and weight. Make sure to index 'statline' correctly so that the subset of the list represents the target/dependent variable. This applies in the for index, statline in enumerate() loop in preprocessing_training_functions.py.
--> On this note, review feature selection for GP and ATOI projections. Make sure that the statistics.mean['index'] 'index' is adjusted appropriately.

Check why Matthew Tkachuk's projection is so high. Powerplay stats are projected high (G, A1, A2). See if he has strong underlying metrics (rebounds created, rush attempts, etc) or adjust these models using different hyperparameters in model_analysis. Resultant CSV files are available in the CSV Data folder.

Optimize make_projection_df by removing the loop.

Next Steps:
1. Grid search for L1 and L2 regularizations using ~5-fold cross-validation. This should be done separately from other hyperparameter/architecture tuning; after scaling, architecture and epochs have been decided. Possibilities for each L1 and L2 should be something like [0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1].
2. Create other models. Linear regression, random forest, GBQ options using existing dataframes.

!! Add a neural network architecture/hyperparameter catalog CSV file / Turn the storage of model architectures/hyperparameters into a CSV (currently commented in model_analysis.py)
--> Also another document briefing the differences between each NN projector for the different stats (filters, features, etc.)

Consider using cross-validation for model testing?
Consider Bayesian neural network for uncertainty?
Random forest?
SVM?
